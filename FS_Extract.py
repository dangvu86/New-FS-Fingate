from io import StringIO, BytesIO
import streamlit as st
from bs4 import BeautifulSoup
import pandas as pd
import zipfile
import gdown
import tempfile
import os

# C·∫•u h√¨nh giao di·ªán
st.set_page_config(layout="wide")
st.title("FS Fingate")

# H√†m t·∫£i file ZIP t·ª´ Google Drive

def download_zip_from_drive(file_id):
    """Download ZIP file from Google Drive with multiple fallback methods"""
    
    # Method 1: Try gdown with direct download
    try:
        url = f"https://drive.google.com/uc?id={file_id}&export=download"
        with tempfile.NamedTemporaryFile(delete=False, suffix=".zip") as tmp_file:
            st.info(f"ƒêang t·∫£i file t·ª´ Google Drive...")
            gdown.download(url, tmp_file.name, quiet=False)
            
            if os.path.getsize(tmp_file.name) > 0:
                with open(tmp_file.name, "rb") as f:
                    content = f.read()
                os.unlink(tmp_file.name)  # Clean up
                return BytesIO(content)
    except Exception as e:
        st.warning(f"Method 1 failed: {e}")
    
    # Method 2: Try alternative gdown approach
    try:
        with tempfile.NamedTemporaryFile(delete=False, suffix=".zip") as tmp_file:
            st.info(f"Th·ª≠ ph∆∞∆°ng ph√°p thay th·∫ø...")
            gdown.download(f"https://drive.google.com/file/d/{file_id}/view?usp=sharing", 
                          tmp_file.name, quiet=False, fuzzy=True)
            
            if os.path.getsize(tmp_file.name) > 0:
                with open(tmp_file.name, "rb") as f:
                    content = f.read()
                os.unlink(tmp_file.name)  # Clean up
                return BytesIO(content)
    except Exception as e:
        st.warning(f"Method 2 failed: {e}")
    
    # Method 3: Direct requests approach
    try:
        import requests
        session = requests.Session()
        
        st.info("Th·ª≠ t·∫£i tr·ª±c ti·∫øp...")
        
        # First request to get confirmation token
        response = session.get(f"https://drive.google.com/uc?id={file_id}&export=download", 
                              stream=True)
        
        # Check for virus scan warning
        token = None
        for key, value in response.cookies.items():
            if key.startswith('download_warning'):
                token = value
                break
        
        if token:
            params = {'id': file_id, 'confirm': token, 'export': 'download'}
            response = session.get("https://drive.google.com/uc", params=params, stream=True)
        
        if response.status_code == 200 and response.headers.get('content-length', '0') != '0':
            return BytesIO(response.content)
            
    except Exception as e:
        st.error(f"Method 3 failed: {e}")
    
    st.error("Kh√¥ng th·ªÉ t·∫£i file t·ª´ Google Drive. Vui l√≤ng ki·ªÉm tra file ID v√† quy·ªÅn truy c·∫≠p.")
    return None


# H√†m tr√≠ch xu·∫•t b·∫£ng t·ª´ HTML
def extract_tables_from_html(html_content):
    soup = BeautifulSoup(html_content, "html.parser")
    tables = soup.find_all("table")
    if tables:
        # Method 1: Try pandas read_html with different parsers
        for parser in ['lxml', 'html5lib', 'html.parser']:
            try:
                df = pd.read_html(StringIO(str(tables[0])), flavor=parser)[0]
                if isinstance(df.columns, pd.MultiIndex):
                    df.columns = [' '.join(map(str, col)).strip() for col in df.columns]
                
                # Clean data
                df.iloc[0, 0] = str(df.iloc[0, 0]).replace('.', '').replace(')', '').replace('(', '-').replace(',', '')
                for col in df.columns[1:]:
                    df[col] = df[col].astype(str).str.replace('.', '', regex=False)
                    df[col] = df[col].astype(str).str.replace(')', '', regex=False)
                    df[col] = df[col].astype(str).str.replace('(', '-', regex=False)
                    df[col] = df[col].astype(str).str.replace(',', '', regex=False)
                    df[col] = pd.to_numeric(df[col], errors="coerce")
                return df
            except Exception as e:
                continue
        
        # Method 2: Manual parsing with BeautifulSoup as fallback
        try:
            table = tables[0]
            rows = table.find_all("tr")
            
            table_data = []
            headers = []
            
            for i, row in enumerate(rows):
                cells = row.find_all(["th", "td"])
                row_data = []
                for cell in cells:
                    text = cell.get_text(strip=True)
                    # Clean the data
                    text = text.replace('.', '').replace(')', '').replace('(', '-').replace(',', '')
                    row_data.append(text)
                
                if i == 0:  # Header row
                    headers = row_data
                else:
                    table_data.append(row_data)
            
            if headers and table_data:
                df = pd.DataFrame(table_data, columns=headers)
                
                # Convert numeric columns
                for col in df.columns[1:]:  # Skip first column (descriptions)
                    df[col] = pd.to_numeric(df[col], errors="coerce")
                
                return df
            else:
                return "Kh√¥ng th·ªÉ parse table structure"
                
        except Exception as e:
            return f"L·ªói manual parsing: {e}"
    return "Kh√¥ng t√¨m th·∫•y b·∫£ng n√†o trong file HTML."

# ID c·ªßa file ZIP tr√™n Google Drive
drive_file_id = "1A0yeEBAvLkX64PlatHboPAHhHVIcJICw"

# T·∫£i file ZIP
st.info("üîÑ ƒêang t·∫£i d·ªØ li·ªáu t√†i ch√≠nh m·∫∑c ƒë·ªãnh...")
uploaded_file = download_zip_from_drive(drive_file_id)

html_tables = {}

# X·ª≠ l√Ω file ZIP
if uploaded_file is not None:
    st.success("‚úÖ T·∫£i file th√†nh c√¥ng!")
    st.info("üìä ƒêang x·ª≠ l√Ω d·ªØ li·ªáu...")
else:
    st.error("‚ùå Kh√¥ng th·ªÉ t·∫£i file. Vui l√≤ng th·ª≠ l·∫°i sau.")
    st.stop()

if uploaded_file is not None:
    try:
        with zipfile.ZipFile(uploaded_file, "r") as zip_ref:
            all_files = zip_ref.namelist()
            st.info(f"üìÅ T√¨m th·∫•y {len(all_files)} files trong ZIP: {', '.join(all_files[:5])}{'...' if len(all_files) > 5 else ''}")
            
            html_files = [f for f in all_files if f.lower().endswith(".html")]
            st.info(f"üìÑ T√¨m th·∫•y {len(html_files)} HTML files: {', '.join(html_files)}")
            
            if not html_files:
                st.warning("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file HTML n√†o trong ZIP.")
                st.info("Danh s√°ch t·∫•t c·∫£ files:")
                for f in all_files:
                    st.write(f"- {f}")
            
            for html_file in html_files:
                st.info(f"üîÑ ƒêang x·ª≠ l√Ω {html_file}...")
                with zip_ref.open(html_file) as file:
                    html_content = file.read().decode("utf-8")
                    result = extract_tables_from_html(html_content)
                    html_tables[html_file] = result
                    
                    if isinstance(result, pd.DataFrame):
                        st.success(f"‚úÖ X·ª≠ l√Ω th√†nh c√¥ng {html_file} - {len(result)} rows")
                    else:
                        st.warning(f"‚ö†Ô∏è {html_file}: {result}")
                        
    except zipfile.BadZipFile:
        st.error("File t·∫£i v·ªÅ kh√¥ng ph·∫£i l√† file ZIP h·ª£p l·ªá.")
    except Exception as e:
        st.error(f"L·ªói khi x·ª≠ l√Ω ZIP file: {e}")
else:
    st.info("Kh√¥ng th·ªÉ t·∫£i file ZIP t·ª´ Google Drive.")

# Hi·ªÉn th·ªã v√† xu·∫•t b·∫£ng
st.info(f"üìä T·ªïng s·ªë b·∫£ng ƒë∆∞·ª£c x·ª≠ l√Ω: {len(html_tables)}")

if html_tables:
    st.success("üéâ T√¨m th·∫•y d·ªØ li·ªáu! ƒêang hi·ªÉn th·ªã b·∫£ng...")
else:
    st.error("‚ùå Kh√¥ng t√¨m th·∫•y b·∫£ng d·ªØ li·ªáu n√†o.")
    st.info("C√≥ th·ªÉ nguy√™n nh√¢n:")
    st.write("- File ZIP kh√¥ng ch·ª©a file HTML")
    st.write("- File HTML kh√¥ng c√≥ b·∫£ng d·ªØ li·ªáu")
    st.write("- L·ªói trong qu√° tr√¨nh x·ª≠ l√Ω")
    st.stop()

if html_tables:
    output_all = BytesIO()
    with pd.ExcelWriter(output_all, engine='xlsxwriter') as writer:
        for name, df in html_tables.items():
            if isinstance(df, pd.DataFrame):
                sheet_name = name[:31]
                df.to_excel(writer, index=False, sheet_name=sheet_name)
                workbook = writer.book
                worksheet = writer.sheets[sheet_name]
                for i, col in enumerate(df.columns):
                    column_len = max(df[col].astype(str).map(len).max(), len(str(col))) + 2
                    align_format = workbook.add_format({'align': 'left' if i == 0 else 'right'})
                    worksheet.set_column(i, i, column_len, align_format)
    output_all.seek(0)

    top_col1, top_col2 = st.columns([5, 1])
    with top_col2:
        st.download_button(
            label="üì• T·∫£i t·∫•t c·∫£ b·∫£ng",
            data=output_all.getvalue(),
            file_name="all_tables.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        )

    tabs = st.tabs(list(html_tables.keys()))
    for tab, name in zip(tabs, html_tables.keys()):
        with tab:
            table = html_tables[name]
            if isinstance(table, pd.DataFrame):
                df_formatted = table.copy()
                for col in df_formatted.select_dtypes(include='number').columns:
                    df_formatted[col] = df_formatted[col].apply(lambda x: f"{int(x):,}" if pd.notnull(x) else "")
                df_formatted.columns = df_formatted.columns.map(str)
                styles = [{'selector': 'th', 'props': [('text-align', 'center')]}]
                styles.append({'selector': 'td.col0', 'props': [('text-align', 'left')]} )
                for i in range(1, len(df_formatted.columns)):
                    styles.append({'selector': f'td.col{i}', 'props': [('text-align', 'right')]} )
                styled_table = df_formatted.style.set_table_styles(styles).set_table_attributes('style="font-size:12px;"')
                st.markdown(styled_table.to_html(), unsafe_allow_html=True)
            else:
                st.error(table)
else:
    st.info("Kh√¥ng t√¨m th·∫•y b·∫£ng n√†o trong file ZIP ho·∫∑c l·ªói khi x·ª≠ l√Ω file.")